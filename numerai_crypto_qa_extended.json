{
  "source": "numerai_signals_crypto_extended",
  "created_at": "2025-11-08",
  "total_pairs": 27,
  "description": "Extended Numerai Signals Q&A covering LightGBM, neutralization, alpha scoring, feature engineering, and crypto adaptations",
  "qa_pairs": [
    {
      "pair_id": "numerai_crypto_004",
      "topic": "LightGBM hyperparameter tuning cryptocurrency machine learning",
      "question": "How should LightGBM hyperparameters be tuned for cryptocurrency price prediction based on Numerai Signals tournament best practices?",
      "answer": "LightGBM (Light Gradient Boosting Machine) is a gradient boosting framework optimized for speed and performance. Numerai Signals uses specific hyperparameters for stock prediction that can be adapted for cryptocurrency markets. Here's a comprehensive guide:\n\n**Numerai Signals LightGBM Configuration:**\n```python\nimport lightgbm as lgb\n\nmodel = lgb.LGBMRegressor(\n  n_estimators=2000,      # Number of boosting rounds\n  learning_rate=0.01,     # Step size shrinkage\n  max_depth=5,            # Maximum tree depth\n  num_leaves=2**5-1,      # 31 leaves per tree\n  colsample_bytree=0.1    # Feature sampling ratio\n)\n```\n\n**Hyperparameter Analysis:**\n\n**1. n_estimators (2000)**\n- **Stock markets:** 2000 iterations with lr=0.01 prevents overfitting\n- **Crypto adaptation:** Consider 3000-5000 iterations\n- **Reasoning:** Crypto has 24/7 data (more samples per time period)\n- **Risk:** Higher values = longer training, but better pattern capture\n- **Early stopping:** Use `early_stopping_rounds=50` to prevent overfitting\n\n```python\n# Crypto-adapted with early stopping\nmodel_crypto = lgb.LGBMRegressor(\n    n_estimators=5000,\n    learning_rate=0.01,\n    early_stopping_rounds=50  # Stop if no improvement for 50 rounds\n)\n\nmodel_crypto.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    eval_metric='rmse'\n)\n```\n\n**2. learning_rate (0.01)**\n- **Numerai choice:** Low learning rate (0.01) for stable convergence\n- **Crypto markets:** Consider 0.005-0.02 range\n- **Higher volatility → lower learning rate**\n- **Formula:** learning_rate = 0.1 / sqrt(n_estimators)\n\n**Learning Rate vs N_estimators Tradeoff:**\n- lr=0.1, n_est=200 → Fast, but overfits\n- lr=0.01, n_est=2000 → Numerai's choice (stable)\n- lr=0.005, n_est=5000 → Crypto high-frequency (very stable)\n\n**3. max_depth (5)**\n- **Interpretation:** Trees can split up to 5 levels deep\n- **Prevents:** Overfitting to noise in financial data\n- **Crypto adaptation:** 5-7 (crypto patterns may need deeper trees)\n- **Warning:** max_depth >10 usually overfits in financial markets\n\n**Depth Impact on Crypto:**\n```python\n# Test different depths\nfor depth in [3, 5, 7, 10]:\n    model = lgb.LGBMRegressor(max_depth=depth, n_estimators=1000)\n    model.fit(X_train, y_train)\n    val_score = model.score(X_val, y_val)\n    print(f'Depth {depth}: Validation R² = {val_score:.4f}')\n\n# Typical results for crypto:\n# Depth 3: 0.4521 (underfitting)\n# Depth 5: 0.5834 (good balance)\n# Depth 7: 0.5912 (marginal improvement)\n# Depth 10: 0.5201 (overfitting!)\n```\n\n**4. num_leaves (31 = 2^5-1)**\n- **Constraint:** num_leaves < 2^max_depth\n- **Numerai:** Uses maximum leaves for depth=5\n- **Crypto adaptation:** Consider 15-63 range\n- **More leaves:** Better fit, higher overfitting risk\n\n**Leaves vs Depth Relationship:**\n- max_depth=5 → max num_leaves = 32\n- Numerai uses 31 (conservative, prevents exact 2^n overfitting)\n- Crypto: Try 23 (2^5-9) for more regularization\n\n**5. colsample_bytree (0.1)**\n- **Critical parameter:** Only 10% of features used per tree!\n- **Purpose:** Feature bagging reduces variance dramatically\n- **Numerai insight:** Prevents over-reliance on single features\n- **Crypto adaptation:** 0.1-0.3 (crypto has fewer features typically)\n\n**Feature Sampling Impact:**\n```python\n# Numerai: 23 features, colsample=0.1 → ~2 features per tree\n# Each tree sees different feature subset → ensemble diversity\n\n# Crypto example with 50 features\nmodel_crypto = lgb.LGBMRegressor(\n    colsample_bytree=0.2,  # 10 features per tree\n    n_estimators=3000\n)\n# Result: 3000 trees × 10 features = 30,000 feature evaluations\n# But only 50 unique features → robust averaging\n```\n\n**Crypto-Specific Hyperparameter Extensions:**\n\n**6. reg_alpha (L1 regularization) - NOT in Numerai example**\n```python\nmodel_crypto = lgb.LGBMRegressor(\n    n_estimators=3000,\n    learning_rate=0.01,\n    max_depth=5,\n    num_leaves=31,\n    colsample_bytree=0.15,\n    reg_alpha=0.1,  # L1 regularization (Lasso)\n    reg_lambda=0.1  # L2 regularization (Ridge)\n)\n```\n- **L1 (alpha):** Encourages sparse solutions (some weights → 0)\n- **L2 (lambda):** Shrinks weights toward zero\n- **Crypto benefit:** Reduces noise from wash trading, bot activity\n\n**7. min_child_samples (minimum data in leaf)**\n```python\nmodel_crypto = lgb.LGBMRegressor(\n    min_child_samples=20,  # At least 20 samples per leaf\n    min_child_weight=0.001 # Minimum sum of hessian in child\n)\n```\n- **Default:** min_child_samples=20\n- **Crypto high-frequency:** Increase to 50-100\n- **Prevents:** Overfitting to individual candles/trades\n\n**Complete Crypto-Optimized Configuration:**\n\n```python\nimport lightgbm as lgb\nfrom sklearn.model_selection import TimeSeriesSplit\n\n# Numerai-inspired crypto configuration\nmodel_crypto = lgb.LGBMRegressor(\n    # Core parameters (Numerai-based)\n    n_estimators=5000,\n    learning_rate=0.01,\n    max_depth=6,  # Slightly deeper for crypto complexity\n    num_leaves=40,  # More leaves for crypto patterns\n    colsample_bytree=0.15,  # 15% feature sampling\n    \n    # Regularization (crypto-specific)\n    reg_alpha=0.05,  # L1 regularization\n    reg_lambda=0.05,  # L2 regularization\n    min_child_samples=50,  # Prevent leaf overfitting\n    \n    # Speed optimizations\n    n_jobs=-1,  # Use all CPU cores\n    verbose=-1,  # Suppress output\n    \n    # Crypto-specific\n    random_state=42,  # Reproducibility\n    importance_type='gain'  # Feature importance metric\n)\n\n# Time-series cross-validation (critical for crypto)\ntscv = TimeSeriesSplit(n_splits=5)\n\nfor train_idx, val_idx in tscv.split(X):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model_crypto.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        eval_metric='rmse',\n        early_stopping_rounds=100,\n        verbose=100\n    )\n```\n\n**Hyperparameter Tuning Strategy:**\n\n**Method 1: Grid Search (computationally expensive)**\n```python\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'learning_rate': [0.005, 0.01, 0.02],\n    'max_depth': [5, 6, 7],\n    'num_leaves': [31, 40, 50],\n    'colsample_bytree': [0.1, 0.15, 0.2]\n}\n\ngrid_search = GridSearchCV(\n    lgb.LGBMRegressor(n_estimators=1000),\n    param_grid,\n    cv=TimeSeriesSplit(n_splits=3),\n    scoring='neg_mean_squared_error',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\nprint(f'Best params: {grid_search.best_params_}')\n```\n\n**Method 2: Optuna (Bayesian optimization - recommended)**\n```python\nimport optuna\n\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 1000, 5000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'num_leaves': trial.suggest_int('num_leaves', 15, 100),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.5),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.5)\n    }\n    \n    model = lgb.LGBMRegressor(**params)\n    model.fit(X_train, y_train)\n    return model.score(X_val, y_val)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\nprint(f'Best params: {study.best_params}')\nprint(f'Best score: {study.best_value}')\n```\n\n**Validation Strategy (Numerai-inspired):**\n\n**Numerai approach:** 651 validation dates, predict each independently\n**Crypto adaptation:**\n\n```python\nimport pandas as pd\nfrom datetime import timedelta\n\n# Walk-forward validation\nvalidation_results = []\n\nfor date in pd.date_range(start='2023-01-01', end='2024-01-01', freq='7D'):\n    # Train on data up to this date\n    train_data = crypto_df[crypto_df['date'] < date]\n    val_data = crypto_df[(crypto_df['date'] >= date) & \n                         (crypto_df['date'] < date + timedelta(days=7))]\n    \n    if len(val_data) == 0:\n        continue\n    \n    model.fit(train_data[features], train_data['target'])\n    predictions = model.predict(val_data[features])\n    \n    val_score = mean_squared_error(val_data['target'], predictions)\n    validation_results.append({\n        'date': date,\n        'mse': val_score,\n        'n_samples': len(val_data)\n    })\n\n# Analyze temporal stability\nval_df = pd.DataFrame(validation_results)\nprint(f'Mean MSE: {val_df[\"mse\"].mean():.4f}')\nprint(f'Std MSE: {val_df[\"mse\"].std():.4f}')\nprint(f'Sharpe-like metric: {val_df[\"mse\"].mean() / val_df[\"mse\"].std():.2f}')\n```\n\n**Key Insights from Numerai:**\n\n1. **Conservative hyperparameters win:** Low learning rate + high n_estimators\n2. **Feature sampling is critical:** colsample_bytree=0.1 prevents overfitting\n3. **Regularization through architecture:** max_depth=5, num_leaves=31\n4. **No explicit regularization:** Numerai doesn't use reg_alpha/reg_lambda (architecture does it)\n5. **Simplicity over complexity:** Fewer parameters to tune = more robust\n\n**Crypto-Specific Recommendations:**\n\n**For Bitcoin/Ethereum (stable, liquid):**\n- Use Numerai parameters almost exactly\n- Maybe increase n_estimators to 3000\n\n**For Large-cap Altcoins:**\n- max_depth=6, num_leaves=40\n- reg_alpha=0.05 (add regularization)\n- colsample_bytree=0.15\n\n**For Small-cap/High-volatility:**\n- max_depth=4 (less depth!)\n- num_leaves=15\n- reg_alpha=0.1, reg_lambda=0.1 (strong regularization)\n- min_child_samples=100\n\n**Sources:**\n- Numerai Signals v2.1 example notebook\n- LightGBM official documentation (https://lightgbm.readthedocs.io/)\n- Ke, Guolin, et al. \"LightGBM: A highly efficient gradient boosting decision tree.\" NeurIPS 2017\n- Cryptocurrency machine learning research (2024-2025)"
    },
    {
      "pair_id": "numerai_crypto_005",
      "topic": "Factor neutralization feature engineering quantitative finance",
      "question": "What is factor neutralization in quantitative finance, and how can this Numerai Signals technique be applied to cryptocurrency trading to improve alpha generation?",
      "answer": "Factor neutralization is a quantitative finance technique that removes unwanted exposures from predictions to isolate true alpha (skill-based returns). Numerai Signals uses this as a core component of their scoring methodology. Here's how it works and adapts to crypto:\n\n**What is Factor Neutralization?**\n\nFactor neutralization orthogonalizes your predictions against common risk factors, ensuring your returns come from unique insights rather than riding broad market movements.\n\n**Mathematical Definition:**\n\nGiven:\n- **predictions** (your signal): P\n- **neutralizers** (risk factors): F₁, F₂, ..., Fₙ\n- **neutralized predictions**: P'\n\nNeutralization process:\n```\n1. Fit linear regression: P = β₁F₁ + β₂F₂ + ... + βₙFₙ + ε\n2. P' = ε (the residuals)\n```\n\nP' is your prediction with factor exposures removed.\n\n**Numerai Signals Neutralization:**\n\n**From the notebook:**\n```python\nfrom numerai_tools.scoring import alpha\n\nalpha_score = alpha(\n    predictions=predictions,        # Your ranked signals\n    neutralizers=neutralizers,      # Risk factors (3.99GB file!)\n    sample_weights=weights,         # Stock importance weights\n    targets=targets                 # Actual future returns\n)\n```\n\n**Numerai's Neutralizers (Stock Market):**\n- Market beta (exposure to S&P 500)\n- Size factor (market cap)\n- Value factor (book-to-price, P/E ratios)\n- Momentum factors (12w, 26w, 52w returns)\n- Sector exposures\n- Country exposures\n- Volatility factor\n\n**Why Neutralization Matters:**\n\n**Example Without Neutralization:**\n```\nYour prediction: \"Buy tech stocks\"\nMarket reality: All tech stocks rise 20% (sector rotation)\nYour P&L: +20%\nYour alpha: 0% (you just rode the sector wave)\n```\n\n**Example With Neutralization:**\n```\nYour prediction: \"Buy tech stocks\" (neutralized to tech sector)\nInterpretation: \"These tech stocks will outperform OTHER tech stocks\"\nMarket reality: Tech sector +20%, your picks +25%\nYour alpha: +5% (actual stock-picking skill!)\n```\n\n**Numerai-Inspired Crypto Neutralization:**\n\n**Crypto Risk Factors to Neutralize:**\n\n1. **Bitcoin Beta** (most important!)\n   - 70-90% of altcoin returns explained by BTC movement\n   - Formula: `β_BTC = Cov(altcoin, BTC) / Var(BTC)`\n\n2. **Market Cap Factor**\n   - Large-cap vs mid-cap vs small-cap behavior\n   - Log(market cap) for normalization\n\n3. **Exchange Factor**\n   - Binance-listed vs Coinbase-listed vs DEX-only\n   - Different liquidity, regulation, user bases\n\n4. **Sector Factor**\n   - DeFi, Layer-1, Layer-2, Meme, Gaming, AI, RWA\n   - Sector rotations are massive in crypto\n\n5. **Volume/Liquidity Factor**\n   - 24h trading volume\n   - Bid-ask spread\n   - Order book depth\n\n6. **Volatility Factor**\n   - 30-day realized volatility\n   - ATR (Average True Range)\n\n**Implementation: Crypto Factor Neutralization**\n\n**Step 1: Calculate Risk Factors**\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Assume crypto_df has: ['symbol', 'date', 'return_7d', 'price', 'volume_24h', 'market_cap']\n\n# Factor 1: Bitcoin Beta (rolling 30-day)\nfor symbol in crypto_df['symbol'].unique():\n    if symbol == 'BTC':\n        crypto_df.loc[crypto_df['symbol'] == symbol, 'btc_beta'] = 1.0\n        continue\n    \n    symbol_data = crypto_df[crypto_df['symbol'] == symbol].copy()\n    btc_data = crypto_df[crypto_df['symbol'] == 'BTC'][['date', 'return_7d']].copy()\n    btc_data.columns = ['date', 'btc_return']\n    \n    merged = symbol_data.merge(btc_data, on='date')\n    \n    # Rolling 30-day beta calculation\n    merged['btc_beta'] = merged.rolling(30).apply(\n        lambda x: np.cov(x['return_7d'], x['btc_return'])[0,1] / np.var(x['btc_return'])\n        if len(x) == 30 else np.nan\n    )\n    \n    crypto_df.loc[crypto_df['symbol'] == symbol, 'btc_beta'] = merged['btc_beta']\n\n# Factor 2: Market Cap (log-normalized)\ncrypto_df['log_market_cap'] = np.log1p(crypto_df['market_cap'])\n\n# Factor 3: Volume Factor (log-normalized)\ncrypto_df['log_volume_24h'] = np.log1p(crypto_df['volume_24h'])\n\n# Factor 4: Volatility (30-day rolling std)\ncrypto_df['volatility_30d'] = (\n    crypto_df.groupby('symbol')['return_7d']\n    .rolling(30).std()\n    .reset_index(level=0, drop=True)\n)\n\n# Factor 5: Sector (one-hot encoded)\ncrypto_df = pd.get_dummies(crypto_df, columns=['sector'], prefix='sector')\n\n# Factor 6: Exchange (one-hot encoded)\ncrypto_df = pd.get_dummies(crypto_df, columns=['primary_exchange'], prefix='exchange')\n```\n\n**Step 2: Neutralize Predictions**\n\n```python\ndef neutralize_predictions(predictions_df, neutralizers_df):\n    \"\"\"\n    Neutralize predictions against risk factors\n    \n    Args:\n        predictions_df: DataFrame with ['symbol', 'date', 'prediction']\n        neutralizers_df: DataFrame with ['symbol', 'date', 'factor1', 'factor2', ...]\n    \n    Returns:\n        predictions_df with added 'prediction_neutralized' column\n    \"\"\"\n    # Merge predictions with neutralizers\n    merged = predictions_df.merge(neutralizers_df, on=['symbol', 'date'])\n    \n    # Select neutralizer columns\n    neutralizer_cols = [col for col in merged.columns if col.startswith(('btc_beta', 'log_', 'volatility', 'sector_', 'exchange_'))]\n    \n    # Fit linear regression for each date (cross-sectional)\n    neutralized_predictions = []\n    \n    for date, group in merged.groupby('date'):\n        X = group[neutralizer_cols].fillna(0)\n        y = group['prediction']\n        \n        # Skip if insufficient data\n        if len(X) < 10:\n            group['prediction_neutralized'] = group['prediction']\n            neutralized_predictions.append(group)\n            continue\n        \n        # Fit linear model\n        model = LinearRegression()\n        model.fit(X, y)\n        \n        # Residuals are neutralized predictions\n        group['prediction_neutralized'] = y - model.predict(X)\n        neutralized_predictions.append(group)\n    \n    result = pd.concat(neutralized_predictions)\n    return result[['symbol', 'date', 'prediction', 'prediction_neutralized']]\n```\n\n**Step 3: Rank Neutralized Predictions (Numerai-style)**\n\n```python\nfrom scipy import stats\n\ndef gaussianize_rank(df, col='prediction_neutralized'):\n    \"\"\"\n    Rank and gaussianize predictions (Numerai's country_ranknorm approach)\n    For crypto: Do this per date, across all symbols\n    \"\"\"\n    df[f'{col}_ranked'] = (\n        df.groupby('date')[col]\n        .rank(pct=True)  # Percentile rank\n        .apply(stats.norm.ppf)  # Inverse normal CDF (gaussianize)\n    )\n    return df\n\n# Apply ranking\npredictions_neutralized = gaussianize_rank(predictions_neutralized)\n```\n\n**Step 4: Calculate Alpha (Neutralized Performance)**\n\n```python\ndef calculate_alpha(predictions_df, returns_df):\n    \"\"\"\n    Calculate alpha: correlation of neutralized predictions with future returns\n    \n    This mimics Numerai's alpha calculation\n    \"\"\"\n    # Merge predictions with future returns\n    merged = predictions_df.merge(\n        returns_df[['symbol', 'date', 'return_7d_forward']],\n        on=['symbol', 'date']\n    )\n    \n    # Calculate correlation per date\n    alpha_by_date = []\n    for date, group in merged.groupby('date'):\n        corr = group['prediction_neutralized_ranked'].corr(group['return_7d_forward'])\n        alpha_by_date.append({'date': date, 'alpha': corr})\n    \n    alpha_df = pd.DataFrame(alpha_by_date)\n    \n    # Summary statistics\n    mean_alpha = alpha_df['alpha'].mean()\n    std_alpha = alpha_df['alpha'].std()\n    sharpe_alpha = mean_alpha / std_alpha if std_alpha > 0 else 0\n    \n    print(f'Mean Alpha: {mean_alpha:.4f}')\n    print(f'Std Alpha: {std_alpha:.4f}')\n    print(f'Alpha Sharpe: {sharpe_alpha:.2f}')\n    \n    return alpha_df\n\nalpha_results = calculate_alpha(predictions_neutralized, crypto_df)\n```\n\n**Complete Workflow Example:**\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats\nimport lightgbm as lgb\n\n# Step 1: Build predictive model (LightGBM)\nfeatures = ['rsi_14', 'macd', 'volume_change', 'price_change_7d']\ntarget = 'return_7d_forward'\n\nmodel = lgb.LGBMRegressor(\n    n_estimators=2000,\n    learning_rate=0.01,\n    max_depth=5,\n    num_leaves=31,\n    colsample_bytree=0.1\n)\n\n# Train on historical data\ntrain_data = crypto_df[crypto_df['date'] < '2024-01-01']\nmodel.fit(train_data[features], train_data[target])\n\n# Step 2: Generate predictions for validation period\nval_data = crypto_df[crypto_df['date'] >= '2024-01-01'].copy()\nval_data['prediction'] = model.predict(val_data[features])\n\n# Step 3: Calculate risk factors\nval_data['btc_beta'] = calculate_btc_beta(val_data)  # Custom function\nval_data['log_market_cap'] = np.log1p(val_data['market_cap'])\nval_data['log_volume'] = np.log1p(val_data['volume_24h'])\nval_data['volatility_30d'] = calculate_volatility(val_data)  # Custom function\n\n# Step 4: Neutralize predictions\nneutralizer_cols = ['btc_beta', 'log_market_cap', 'log_volume', 'volatility_30d']\n\nval_data_neutralized = val_data.copy()\nfor date, group in val_data.groupby('date'):\n    X = group[neutralizer_cols].fillna(0)\n    y = group['prediction']\n    \n    model_neutralize = LinearRegression()\n    model_neutralize.fit(X, y)\n    \n    residuals = y - model_neutralize.predict(X)\n    val_data_neutralized.loc[group.index, 'prediction_neutralized'] = residuals\n\n# Step 5: Rank and evaluate\nval_data_neutralized['pred_ranked'] = (\n    val_data_neutralized.groupby('date')['prediction_neutralized']\n    .rank(pct=True)\n)\n\n# Calculate alpha\nalpha_by_date = []\nfor date, group in val_data_neutralized.groupby('date'):\n    corr = group['pred_ranked'].corr(group[target])\n    alpha_by_date.append({'date': date, 'alpha': corr})\n\nalpha_df = pd.DataFrame(alpha_by_date)\nprint(f'\\nAlpha Sharpe: {alpha_df[\"alpha\"].mean() / alpha_df[\"alpha\"].std():.2f}')\nprint(f'Numerai comparison: Numerai basic model Sharpe = 0.80')\n```\n\n**Key Benefits of Neutralization:**\n\n1. **Removes BTC Correlation:**\n   - Raw signal: 85% correlated with BTC\n   - Neutralized signal: 15% correlated with BTC\n   - Result: True altcoin-specific alpha\n\n2. **Market-Neutral Strategy:**\n   - Can profit in bear markets\n   - Lower drawdowns\n   - Better risk-adjusted returns\n\n3. **Isolates Skill:**\n   - Proves your edge is stock-picking, not market-timing\n   - Important for hedge fund allocations\n\n**Numerai's Key Insight:**\n\n\"Our basic features and factors are really only useful to **remove** exposures to them, not rely on them for prediction.\"\n\nThis means:\n- Don't try to predict using market cap, volume, volatility\n- Instead, use them to neutralize your predictions\n- Bring proprietary data/features for actual prediction\n\n**Crypto Application:**\n\nDon't predict that \"high volume coins will rise\" (that's a volume factor bet).\nInstead predict \"Coin X will outperform similar high-volume coins\" (neutralized alpha).\n\n**Sources:**\n- Numerai Signals v2.1 scoring methodology\n- numerai-tools package documentation\n- Fama-French factor models (academic foundation)\n- Quantitative Portfolio Management research (2024-2025)"
    }
  ]
}
